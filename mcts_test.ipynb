{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05323081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "903e8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def manhattan_distance(a, b):\n",
    "#     return abs(a[0] - b[0]) + abs(a[1] - b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "417c2545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def heuristic_value(env, state):\n",
    "#     # High value near goal, low near ghosts, small penalty for distance\n",
    "#     dist_to_goal = manhattan_distance(state, env.goal_pos)\n",
    "#     ghost_penalty = -5 if state in env.ghost_positions else 0\n",
    "#     return -dist_to_goal + ghost_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60c26424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, state, parent=None, action_from_parent=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.visits = 0\n",
    "        self.total_reward = 0.0\n",
    "        self.untried_actions = None  # lazy init\n",
    "        self.action_from_parent = action_from_parent  # useful when re-rooting\n",
    "\n",
    "    def uct_score(self, C=1.41):\n",
    "        if self.visits == 0:\n",
    "            return float(\"inf\")\n",
    "        avg = self.total_reward / self.visits\n",
    "        explore = C * np.sqrt(np.log(self.parent.visits) / self.visits)\n",
    "        return avg + explore\n",
    "\n",
    "    def best_child(self, C=1.41):\n",
    "        return max(self.children.values(), key=lambda c: c.uct_score(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b478b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTree:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "\n",
    "    def select(self, node):\n",
    "        return max(node.children, key=lambda c: c.uct_score())\n",
    "\n",
    "    def backpropagate(self, node, reward):\n",
    "        # walk up the tree updating stats\n",
    "        while node is not None:\n",
    "            node.update(reward)\n",
    "            node = node.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0df627e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, state, max_depth=200):\n",
    "    \"\"\"Random rollout from state using simulate_step.\"\"\"\n",
    "    total_reward = 0\n",
    "    for _ in range(max_depth):\n",
    "        actions = env.get_valid_actions(state)\n",
    "        if not actions:\n",
    "            break\n",
    "        action = np.random.choice(actions)\n",
    "        state, reward, done = env.simulate_step(state, action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27635194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def simulate(env, node, max_depth=120):\n",
    "    \"\"\"Play randomly from node.state until terminal or depth cutoff.\"\"\"\n",
    "    sim_env = copy.deepcopy(env)\n",
    "    sim_env.player_pos = node.state\n",
    "    total_reward = 0\n",
    "    for _ in range(max_depth):\n",
    "        action = np.random.choice(sim_env.action_space)\n",
    "        _, reward, done = sim_env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9525c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_search(env, root, iterations=500, C=1.41):\n",
    "    \"\"\"Perform MCTS from the given root node, return best action.\"\"\"\n",
    "    for _ in range(iterations):\n",
    "        node = root\n",
    "        state = node.state\n",
    "\n",
    "        # 1. Selection: descend tree until a leaf or unexpanded action\n",
    "        while node.untried_actions == [] and node.children:\n",
    "            node = node.best_child(C)\n",
    "            state = node.state\n",
    "\n",
    "        # 2. Expansion: expand *one* untried action\n",
    "        if node.untried_actions is None:\n",
    "            node.untried_actions = env.get_valid_actions(state)\n",
    "\n",
    "        if node.untried_actions:\n",
    "            action = node.untried_actions.pop()\n",
    "            next_state, reward, done = env.simulate_step(state, action)\n",
    "            child = TreeNode(next_state, parent=node, action_from_parent=action)\n",
    "            node.children[action] = child\n",
    "            node = child\n",
    "            state = next_state\n",
    "\n",
    "        # 3. Simulation: heuristic-guided rollout\n",
    "        reward = rollout(env, state)\n",
    "\n",
    "        # 4. Backpropagation\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            node.total_reward += reward\n",
    "            node = node.parent\n",
    "\n",
    "    # Best action from root = most visited child\n",
    "    best_action = max(root.children.items(), key=lambda kv: kv[1].visits)[0]\n",
    "    return best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1f0c8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_root_after_move(root, actual_action, new_state):\n",
    "    \"\"\"Reuse the subtree after taking a real action.\"\"\"\n",
    "    if actual_action in root.children:\n",
    "        new_root = root.children[actual_action]\n",
    "        new_root.parent = None\n",
    "        return new_root\n",
    "    else:\n",
    "        return TreeNode(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97a01f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from environment.grid_loader import load_grid\n",
    "from environment.grid_env import GridEnv\n",
    "\n",
    "# grid_filepath = config.ENV_PARAMS.get('grid_filepath', \"grid.npy\")\n",
    "grid_filepath = \"grid.npy\"\n",
    "grid, start_pos, goal_pos, ghost_positions = load_grid(grid_filepath)\n",
    "env = GridEnv(grid, start_pos, goal_pos, ghost_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abbe0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 0: moved RIGHT\n",
      "Action 1: moved DOWN\n",
      "Action 2: moved RIGHT\n",
      "Action 3: moved RIGHT\n",
      "Action 4: moved DOWN\n",
      "Action 5: moved RIGHT\n",
      "Action 6: moved DOWN\n",
      "Action 7: moved RIGHT\n",
      "\n",
      "VICTORY! in 8 steps\n",
      "\n",
      "Game finished with total reward: 3\n"
     ]
    }
   ],
   "source": [
    "player_pos = env.reset()\n",
    "root = TreeNode(player_pos)\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "i = 0\n",
    "\n",
    "while not done:\n",
    "    action = monte_carlo_search(env, root, iterations=500, C=1.41)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    # Re-root the tree at the chosen child\n",
    "    root = update_root_after_move(root, action, next_state)\n",
    "    print(f\"Action {i}: moved\", next(key for key, value in config.ACTIONS.items() if value == action))\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    if reward == config.REWARDS['GOAL']:\n",
    "        print(f'\\nVICTORY! in {i} steps\\n')\n",
    "    elif reward == config.REWARDS['GHOST']:\n",
    "        print(f'\\nDEFEAT! in {i} steps\\n')\n",
    "\n",
    "print(\"Game finished with total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f01479",
   "metadata": {},
   "source": [
    "# Average Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea92a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = 0\n",
    "# i = 0\n",
    "# wins = 0\n",
    "# while i < 100:\n",
    "#     player_pos = env.reset()\n",
    "#     root = TreeNode(player_pos)\n",
    "#     done = False\n",
    "\n",
    "#     while not done:\n",
    "#         action = monte_carlo_search(env, root, iterations=1000, C=2)\n",
    "#         next_state, reward, done = env.step(action)\n",
    "#         total_reward += reward\n",
    "#         # Re-root the tree at the chosen child\n",
    "#         root = update_root_after_move(root, action, next_state)\n",
    "\n",
    "#         i += 1\n",
    "\n",
    "#         if reward == config.REWARDS['GOAL']:\n",
    "#             wins += 1\n",
    "#         if reward == config.REWARDS['GHOST']:\n",
    "#             losses += 1\n",
    "\n",
    "# print(wins/100)\n",
    "# print(losses/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d19769",
   "metadata": {},
   "source": [
    "# Scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5eafac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.helpers import make_grid\n",
    "# from visualisation.pygame_renderer import PygameRenderer\n",
    "\n",
    "# grid = make_grid(50, 40, 300, 0.2)\n",
    "\n",
    "# grid_mapping = config.GRID_MAPPING\n",
    "\n",
    "# start_pos = np.where(grid==grid_mapping['PLAYER'])\n",
    "# start_pos = (start_pos[0][0], start_pos[1][0])\n",
    "\n",
    "# goal_pos = np.where(grid==grid_mapping['GOAL'])\n",
    "# goal_pos = (goal_pos[0][0], goal_pos[1][0])\n",
    "\n",
    "# ghost_positions = np.where(grid==config.GRID_MAPPING['GHOST'])\n",
    "# ghost_positions = [(int(r), int(c)) for r, c in zip(ghost_positions[0], ghost_positions[1])]\n",
    "\n",
    "# env = GridEnv(grid, start_pos, goal_pos, ghost_positions)\n",
    "# renderer = PygameRenderer(env.height, env.width)\n",
    "\n",
    "# renderer.init_display(\"RL vs Pacman (Human Player)\")\n",
    "\n",
    "# current_display_grid = env.get_grid_snapshot()\n",
    "# renderer.render(current_display_grid, player_pos=start_pos)\n",
    "\n",
    "# np.save('big_grid.npy', grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c534a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Random Agent: 100%|██████████| 1000/1000 [00:13<00:00, 76.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Random Agent Performance =====\n",
      "Games played: 1000\n",
      "Win rate: 1.10% (11/1000)\n",
      "Ghost collision rate: 98.90% (989/1000)\n",
      "Average steps per episode: 32.49\n",
      "Average reward per episode: -41.27\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Get all valid actions from current state\n",
    "        valid_actions = self.env.get_valid_actions(state)\n",
    "\n",
    "        # If no valid actions (shouldn't happen in proper grid), pick any action\n",
    "        if not valid_actions:\n",
    "            valid_actions = list(config.ACTIONS.values())\n",
    "\n",
    "        # Pick a random valid action\n",
    "        return np.random.choice(valid_actions)\n",
    "\n",
    "# Function to run a single episode with the random agent\n",
    "def run_random_agent_episode(env, agent):\n",
    "    player_pos = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(player_pos)\n",
    "        player_pos, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    # Determine game outcome\n",
    "    win = reward == config.REWARDS['GOAL']\n",
    "    ghost_collision = reward == config.REWARDS['GHOST']\n",
    "\n",
    "    return {\n",
    "        'win': win,\n",
    "        'ghost_collision': ghost_collision,\n",
    "        'steps': steps,\n",
    "        'total_reward': total_reward\n",
    "    }\n",
    "\n",
    "# Test the random agent for 100 games\n",
    "def evaluate_random_agent(env, num_episodes=100):\n",
    "    random_agent = RandomAgent(env)\n",
    "\n",
    "    wins = 0\n",
    "    ghost_collisions = 0\n",
    "    total_steps = 0\n",
    "    total_rewards = 0\n",
    "\n",
    "    # Run episodes with progress bar\n",
    "    for _ in tqdm(range(num_episodes), desc=\"Testing Random Agent\"):\n",
    "        result = run_random_agent_episode(env, random_agent)\n",
    "\n",
    "        wins += int(result['win'])\n",
    "        ghost_collisions += int(result['ghost_collision'])\n",
    "        total_steps += result['steps']\n",
    "        total_rewards += result['total_reward']\n",
    "\n",
    "        # Small delay to prevent CPU overload\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    # Calculate statistics\n",
    "    win_rate = wins / num_episodes\n",
    "    ghost_collision_rate = ghost_collisions / num_episodes\n",
    "    avg_steps = total_steps / num_episodes\n",
    "    avg_reward = total_rewards / num_episodes\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n===== Random Agent Performance =====\")\n",
    "    print(f\"Games played: {num_episodes}\")\n",
    "    print(f\"Win rate: {win_rate:.2%} ({wins}/{num_episodes})\")\n",
    "    print(f\"Ghost collision rate: {ghost_collision_rate:.2%} ({ghost_collisions}/{num_episodes})\")\n",
    "    print(f\"Average steps per episode: {avg_steps:.2f}\")\n",
    "    print(f\"Average reward per episode: {avg_reward:.2f}\")\n",
    "    print(\"===================================\")\n",
    "\n",
    "    return {\n",
    "        'win_rate': win_rate,\n",
    "        'ghost_collision_rate': ghost_collision_rate,\n",
    "        'avg_steps': avg_steps,\n",
    "        'avg_reward': avg_reward\n",
    "    }\n",
    "\n",
    "# Run the evaluation\n",
    "random_agent_results = evaluate_random_agent(env, num_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5b2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
